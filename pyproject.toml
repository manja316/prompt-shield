[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "ai-injection-guard"
version = "0.1.0"
description = "Lightweight prompt injection detector for LLM applications. Blocks injection attacks before they reach your model."
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.8"
keywords = ["llm", "ai", "security", "prompt-injection", "jailbreak", "anthropic", "openai", "safety"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Topic :: Security",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
dependencies = []  # Zero runtime dependencies â€” pure stdlib

[project.optional-dependencies]
dev = ["pytest>=7.0", "pytest-cov"]

[project.scripts]
prompt-shield = "prompt_shield.tools.cli:main"

[project.urls]
Homepage = "https://github.com/LuciferForge/prompt-shield"
Repository = "https://github.com/LuciferForge/prompt-shield"
Issues = "https://github.com/LuciferForge/prompt-shield/issues"

[tool.hatch.build.targets.wheel]
packages = ["prompt_shield"]

[tool.pytest.ini_options]
testpaths = ["tests"]
